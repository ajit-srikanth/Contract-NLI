{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Entailment & Contradiction models on the same data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "import wandb \n",
    "import csv\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "# from transformers import DistilBertTokenizer\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# from tabulate import tabulate\n",
    "# from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import ContractNLIDataset, ContractNLIDatasetTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENTAILMENT = 0\n",
    "CONTRADICTION = 1\n",
    "NOT_MENTIONED = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_folder = \"../dataset/contract-nli\"\n",
    "csv_folder = \"../dataset/csv/all_labels\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(f'{csv_folder}/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    968\n",
       "2    903\n",
       "1    220\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "\n",
    "folder_entailment_model = 'all-MiniLM-L6-v2'\n",
    "folder_contradiction_model = 'miniLM'\n",
    "\n",
    "entailment_model = AutoModelForSequenceClassification.from_pretrained(f'./{folder_entailment_model}_entailment')\n",
    "contradiction_model = AutoModelForSequenceClassification.from_pretrained(f'./{folder_contradiction_model}_contradiction')\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    }
   ],
   "source": [
    "max_length = tokenizer.model_max_length\n",
    "print(max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = test_df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_data(data):\n",
    "    return tokenizer(text=data['concatenated_spans'].tolist(), text_pair=data['hypothesis'].tolist(), truncation=True, padding=\"max_length\", max_length=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_encodings = tokenize_data(test_df)\n",
    "test_dataset = ContractNLIDatasetTest(test_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_entailment = Trainer(\n",
    "    model=entailment_model,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "trainer_contradiction = Trainer(\n",
    "    model=contradiction_model,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the Trainer.predict() method to get predictions\n",
    "predictions_entailment = trainer_entailment.predict(test_dataset)\n",
    "pred_labels_entailment = predictions_entailment.argmax(axis=1)\n",
    "\n",
    "predictions_contradiction = trainer_contradiction.predict(test_dataset)\n",
    "pred_labels_contradiction = predictions_contradiction.argmax(axis=1)\n",
    "\n",
    "classes = {\n",
    "    \"00\": {\"ENTAILMENT\": 0, \"CONTRADICTION\": 0, \"NOT_MENTIONED\": 0},\n",
    "    \"01\": {\"ENTAILMENT\": 0, \"CONTRADICTION\": 0, \"NOT_MENTIONED\": 0},\n",
    "    \"10\": {\"ENTAILMENT\": 0, \"CONTRADICTION\": 0, \"NOT_MENTIONED\": 0},\n",
    "    \"11\": {\"ENTAILMENT\": 0, \"CONTRADICTION\": 0, \"NOT_MENTIONED\": 0},\n",
    "}\n",
    "\n",
    "\n",
    "for pred_entailment, pred_contradiction, true_label in zip(pred_labels_entailment, pred_labels_contradiction, test_labels,):\n",
    "    if true_label == 0:\n",
    "        classes[f\"{pred_entailment}{pred_contradiction}\"][\"ENTAILMENT\"] += 1\n",
    "    \n",
    "    elif true_label == 1:\n",
    "        classes[f\"{pred_entailment}{pred_contradiction}\"][\"CONTRADICTION\"] += 1\n",
    "        \n",
    "    elif true_label == 2:\n",
    "        classes[f\"{pred_entailment}{pred_contradiction}\"][\"NOT_MENTIONED\"] += 1\n",
    "\n",
    "\n",
    "for key, value in classes.items():\n",
    "    print(f\"Predicted: {key}\")\n",
    "    print(f\"ENTAILMENT: {value['ENTAILMENT']}\")\n",
    "    print(f\"CONTRADICTION: {value['CONTRADICTION']}\")\n",
    "    print(f\"NOT_MENTIONED: {value['NOT_MENTIONED']}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "label_mapping = {\n",
    "    (0, 0): NOT_MENTIONED,\n",
    "    (1, 0): ENTAILMENT,\n",
    "    (0, 1): CONTRADICTION,\n",
    "    (1, 1): ENTAILMENT\n",
    "}\n",
    "\n",
    "# Use list comprehension with mapping\n",
    "final_predictions = [\n",
    "    label_mapping[(pred_entailment, pred_contradiction)]\n",
    "    for pred_entailment, pred_contradiction in zip(pred_labels_entailment, pred_labels_contradiction)\n",
    "]\n",
    "\n",
    "\n",
    "# Compute and display the confusion matrix\n",
    "confusion_mat = confusion_matrix(test_labels, final_predictions)\n",
    "print(\"Confusion Matrix:\\n\", confusion_mat)\n",
    "\n",
    "# Display classification report\n",
    "class_names = [\"ENTAILMENT\", \"CONTRADICTION\", \"NOT MENTIONED\"]\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(classification_report(test_labels, final_predictions, target_names=class_names))\n",
    "\n",
    "# Identify incorrect predictions\n",
    "incorrect_predictions = [\n",
    "    i for i, (true, pred) in enumerate(zip(test_labels, final_predictions)) if true != pred\n",
    "]\n",
    "\n",
    "print(f\"\\nError Analysis:\")\n",
    "print(f\"Number of incorrect predictions: {len(incorrect_predictions)} out of {len(test_labels)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anlpenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
